import streamlit as st
import torch
from transformers import MBartForConditionalGeneration, MBartTokenizer
from huggingface_hub import hf_hub_download

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö
st.set_page_config(page_title="ABSA Sentiment Analysis", layout="centered")

st.markdown(
    """
    <style>
        .container {
            max-width: 700px;
            margin: auto;
            border-radius: 10px;
            background-color: #f9f9f9;
            box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);
        }
        .content {
            text-align: justify;
            line-height: 1.6;
        }
        .under {
            text-decoration-line: underline;
            text-decoration-style: double;
        }
    </style>
    """,
    unsafe_allow_html=True,
)

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å Hugging Face
@st.cache_resource
def load_model():
    try:
        model_path = hf_hub_download(repo_id="firstmetis/absa_it", filename="model.pth")

        model = MBartForConditionalGeneration.from_pretrained("facebook/mbart-large-50")

        tokenizer = MBartTokenizer.from_pretrained("facebook/mbart-large-50")
        special_tokens = ['<SYMBOL>', '<ASPECT>', '<OPINION>', '<POS>', '<NEG>', '<NEU>']
        tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})

        model.resize_token_embeddings(len(tokenizer))
        model.load_state_dict(torch.load(model_path, map_location="cpu"))

        model.eval()
        return model, tokenizer

    except Exception as e:
        st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Ç‡∏ì‡∏∞‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•: {e}")
        return None, None

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ tokenizer
model, tokenizer = load_model()

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
if model is not None and tokenizer is not None:
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    def generate_text(input_text):
        input_ids = tokenizer(input_text, return_tensors="pt", truncation=True, padding=True, max_length=512).input_ids
        input_ids = input_ids.to(device)

        with torch.no_grad():
            outputs = model.generate(
                input_ids,
                num_beams=4,
                do_sample=True,
                temperature=1.2,
                top_k=50,
                top_p=0.95,
                num_return_sequences=4,
                max_length=50,
                return_dict_in_generate=True,  
                output_scores=True  
            )

        sequences = outputs.sequences
        scores = outputs.scores  

        output_texts = [
            tokenizer.decode(seq, skip_special_tokens=False).replace("</s>", "").replace("<pad>", "").strip()
            for seq in sequences
        ]

        confidences = []
        for seq_scores in scores:
            last_token_logits = seq_scores[-1]  
            probs = torch.nn.functional.softmax(last_token_logits, dim=-1)  
            confidence = probs.max().item()  
            confidences.append(confidence)

        return list(zip(output_texts, confidences))

    # ‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡πá‡∏ö‡πÅ‡∏≠‡∏õ
    st.title("üìå Aspect-based Sentiment Analysis (ABSA)")
    st.markdown(
    """
    <div class='content'>
        <h4>üìç ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå</h4>
        <p>
            &emsp;1. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏û‡∏≤‡∏î‡∏´‡∏±‡∏ß‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏ô‡πÉ‡∏à‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ</br>
            &emsp;&emsp;&emsp;- ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏´‡∏∏‡πâ‡∏ô‡πÑ‡∏ó‡∏¢‡πÉ‡∏ô‡∏õ‡∏µ ‡∏û.‡∏®.2566-2567</br>
            &emsp;&emsp;&emsp;- ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏´‡∏∏‡πâ‡∏ô‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏´‡∏∏‡πâ‡∏ô‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô</br>
            &emsp;&emsp;&emsp;- ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏´‡∏∏‡πâ‡∏ô‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡πà‡∏≠‡∏¢</br>
            &emsp;&emsp;&emsp;   <u class="under">‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á</u> : TISCO ‡∏õ‡∏±‡∏ô‡∏ú‡∏•‡∏î‡∏µ ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏∞‡∏™‡∏° ‡∏ö‡∏•.‡∏î‡∏µ‡∏ö‡∏µ‡πÄ‡∏≠‡∏™‡∏Ø‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πâ‡∏≤ 118 ‡∏ö.</br>
            &emsp;2. ‡∏ô‡∏≥‡∏û‡∏≤‡∏î‡∏´‡∏±‡∏ß‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏™‡πà‡∏•‡∏á‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á</br>
            &emsp;3. ‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° Apply ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
        </p>
    </div>
    """,
    unsafe_allow_html=True,
)
    st.markdown("‡πÉ‡∏™‡πà‡∏û‡∏≤‡∏î‡∏´‡∏±‡∏ß‡∏Ç‡πà‡∏≤‡∏ß‡∏´‡∏∏‡πâ‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Sentiment")

    # ‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    user_input = st.text_input("‚úçÔ∏è ‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ :", "")

    # ‡∏õ‡∏∏‡πà‡∏° Apply
    if st.button("Apply"):
        if user_input:
            results = generate_text(user_input)

            # üîπ ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à
            st.markdown("### üîç ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:")
            for i, (text, confidence) in enumerate(results, 1):
                st.markdown(f"**üîπ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå {i}:** {text}  \nüìå **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à:** {confidence:.2%}")

        else:
            st.warning("‚ö†Ô∏è ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏î Apply")
